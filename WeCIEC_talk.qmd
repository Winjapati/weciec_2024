---
title: "Representing Semantic Relationships in Ancient IE Languages" 
subtitle: "A Pilot Study"
author: "Anton Vinogradov, Gabriel Wallace, and Andrew Byrd"
institute: "University of Kentucky" 
date: "Friday, October 25, 2024"
format: 
  revealjs:
    incremental: false
    extensions:
      - qrcode
editor: visual
---

## Download: Slides, Paper, Data

<br>

::: {style="display: flex; justify-content: center;"}
{{< qrcode https://github.com/Winjapati/weciec_2024/tree/main/WeCIEC_talk_files qr1 width=400 height=400 colorDark='#0011bb' >}}
:::

## Overview of Talk

-   What led us to do this?
-   Tactic #1: WordNet
-   Tactic #2: Reconstructing Word Embeddings using Descendant Languages
-   Future Directions

## Anton sends his regrets

[![](Anton.png){fig-align="center"}](https://linktr.ee/garyyo)

::: {style="display: flex; justify-content: center;"}
<i>*Dr. Anton Vinogradov,(recent!) PhD, Computer Science*</i>
:::

# What led us to do this?

## DERBi PIE

![](Horsey.jpg){fig-align="center"}

::: {style="display: flex; justify-content: center;"}
<i>*Database of Etymological Roots Beginning in PIE*</i>
:::

## DERBi PIE

::: fragment
-   Etymological database, with multiple references.
    -   all of LIV parsed (thanks Thomas Olander!)
    -   half of Pokorny parsed (will finish next July, provided funding)
    -   will finish parsing NIL this December
    -   ultimately: everything (?!?)
-   Have applied for NEH funding, hopefully this will put us closer to the goal of releasing to public a year from now.
:::

## DERBi PIE: Query Searches <!--# Insert DERBi PIE wheel header -->

::: fragment
-   Search Functions Created
    -   Integrated Texts - identify roots, stems, and words in texts
    -   Phonological Search - identify roots, stems, and words by phonological shape (regex)
    -   Morphological Search - identify roots, stems, and words by morphological property (POS, class, gender, etc.)
-   Quickly realized that to identify semantic categories and relationships -- especially ones that make sense across languages -- is ***not*** an easy task
    -   there is no unified classification of semantics
:::

## DERBi PIE: What could this do for us? <!--# Insert DERBi PIE wheel header -->

-   In DERBi PIE, having an integrated semantic system such as this would allow us to provide automated answers to questions such as:

    -   Are certain sound sequences associated with certain meanings or semantic spheres?

    -   Are certain morphological derivations associated with certain meanings or semantics spheres?

    -   How have meanings changed over time into the various branches and daughter languages?

## DERBi PIE: the Problem <!--# Insert DERBi PIE wheel header -->

<br> <br> <br>

<p style="text-align: center;">

This is the problem: how on earth can we do this?

</p>

<br>

<p style="text-align: center;">

And *can* we do this?

</p>

# Tactic #1: WordNet

## WordNet: What is it?

::: {style="display: flex; justify-content: center;"}
[![](wordnet_princeton.png)](https://wordnet.princeton.edu/)
:::

-   A large, organized lexical database of English words
-   Groups words into "synsets" (sets of synonyms) based on meanings

## WordNet: What is it?

::: {style="display: flex; justify-content: center;"}
[![](wordnet_princeton)](https://wordnet.princeton.edu/)
:::

-   Provides relationships between words, such as:
    -   **Synonyms** (similar meanings, e.g. "dog", "pooch")
    -   **Antonyms** (opposite meanings, e.g. "bad", "good")
    -   **Hypernyms** (general terms, e.g., "animal" for "dog")
    -   **Hyponyms** (specific terms, e.g., "dog" for "animal")

## WordNet: Linked WordNets for Ancient Indo-European languages (Zanchi & Ginevra)

::: {style="display: flex; justify-content: center;"}
[![](WordNets){width="90%"}](https://sites.google.com/unipv.it/linked-wordnets/wordnets?authuser=0)
:::

## WordNet: Building One for PIE?

::: {style="display: flex; justify-content: center;"}
![](PIE_wordnet.png)
:::

-   Working with a team of UK CS undergraduates, we mapped a list of PIE roots and words (primarily from Pokorny) onto the English WordNet structure
-   1500 roots successfully mapped, though many are false matches ('golf'?)

## WordNet: Building One for PIE?

-   Numerous substantive difficulties with entries that do not map neatly:
    -   *\*aghlu- (IEW*, p. 8) 'dark cloud; rainy weather': new hyponym (of both 'cloud' and 'weather') needed
    -   *\*ab- (IEW*, p. 1) 'water, river': new hypernym needed? or two mappings?
-   The general idea of English (or any other language) \|\> PIE is flawed, because PIE ≠ English!

## WordNet: Broader Problems

-   **Limited Scope of Meanings**: Doesn’t capture all nuances of word usage
-   **Lack of Context**: Doesn’t account for how context alters word meaning
-   **Not All Languages Have WordNet**: WordNets in some languages are better developed than in others
-   **MUST BE DONE MANUALLY**

# Tactic #2: Reconstructing Word Embeddings using Descendant Languages

## Word Embeddings

-   **Distributional Analysis:** Linguistic items with similar distributions have similar meanings
-   **Vectors:** These are generated based on a given word's distribution in a corpus
-   **Use:** The more similar any two given word's vectors are to each other, the closer those words are in meaning

## Word Embeddings

-   **Example:**
    -   Take a text, like *Beowulf*
    -   Pre-process it (tokenization, lemmatization, etc.)
    -   Generate vectors (word2vec, fastText, etc.)
    -   Compare word vectors through cosine similarities (semantic relatedness)
    -   Results!

## Word Embeddings

![Plots like these can be created with generated vectors!](beowulf.png){fig-align="center"}

## Word Embeddings: Methods

-   X

## Word Embeddings: Methods

-   X

## Word Embeddings: Results

-   X

## Word Embeddings: Results

-   X

## Word Embeddings: Reconstructing a Hyperspace in Languages without a Corpus

-   An analysis of relatedness of lexemes in a language can be done in ancient languages
-   But how does one do this for languages without any known corpus, such as PIE?
-   Well, how do we identify other properties of PIE?

## Word Embeddings : Basic Idea

<br>

::: {style="display: flex; justify-content: center;"}
![](comp_meth_sound.png){fig-align="center"}
:::

-   We take two, similar properties in two related languages, which allow us to infer the approximation of an earlier state in a source language

## Word Embeddings : Basic Idea

::: {style="display: flex; justify-content: center;"}
![](comp_meth_hyperspace.png){fig-align="center"}
:::

-   It is in this way that we propose that the use of word embedding models created by descendant languages, to infer the approximation of an earlier state in the source language

## Word Embeddings: Methods

::: {style="display: flex; justify-content: center;"}
{{< qrcode https://github.com/Winjapati/weciec_2024/tree/main/WeCIEC_talk_files qr1 width=400 height=400 colorDark='#0011bb' >}}
:::

-   As you can imagine, this stuff is complicated, which is why we won't go into much detail about the specific methods;
    -   see Github for four-page paper, code, data, etc.
-   If there are any questions that we can't answer, we'll forward to our main collaborator, Anton, who will be happy to do so

## Word Embeddings, Problem #1: Vectors Across Models

-   Vectors generated for hyperspaces take on arbitrary values when training models
-   You must *align* models (Dev et al., 2021):
    -   identify substructures across language models that remain fixed
    -   we use pre-aligned word embedding models (following Joulin et al., 2018)

## Word Embeddings, Problem #2: Verification

-   So we take pre-aligned hyperspaces, and reconstruct an earlier, source hyperspace based on the hyperspaces provided
-   But how can we trust this methodology?
    -   Obviously can't verify the hyperspace of PIE through analysis of PIE texts!

## Word Embeddings, Problem #2: Verification

![](comp_meth_romance.png){fig-align="center"}

## Word Embeddings: Methods

-   We use existing aligned models of Spanish & French (Joulin et al., 2018) as a source of vector and word information
    -   aligned word vectors taken from Facebook AI Research's fastText (Bojanowski et al., 2017)
-   Words filtered out:
    -   if there is no corresponding word in the other languages
    -   non-vocabulary, including words with non-language characters

## Word Embeddings: Methods

-   Models trained on French & Spanish Wikipedia articles, include both vocabulary and non-words/words containing non-language characters, the latter of which were removed (7.3%, 6.6%, respectively)

-   Remaining words lemmatized, furthering reducing vocabulary by roughly 10%

## Word Embeddings: Methods

-   To relate words together and find common words:
    -   words are translated into each other's respective languages, using Google Translate, with a python translation library deep-translator
    -   the same is done with Latin, using the Latin corpus (from CLTK \[the Tesserae Project\]), which is lemmatized
-   Any word that cannot be lemmatized in Latin (such as Greek words) is removed from the corpus

## Word Embeddings: Calculating \*Latin

![](centroid.png){fig-align="center"}

::: {style="font-size: 0.85em;"}
1.  Identify the centroid of each vector: **language-word center**
2.  Identify the centroid of both **lwc**s -\> **inter-language-word center**
3.  Identify the closest vectors of to the **ilwc** using cosine-distance
4.  Take the average of these two vectors to arrive at the approximate \*Latin-word vector
:::

## Word Embeddings: Results

-   To identify the effectiveness of the method, we 

![](comp_meth_romance.png){fig-align="center"}


## Word Embeddings: NLP Problems

1.  Use of Google translate suboptimal, may result in translation errors; should use either bilingual dictionaries or LLMs (think GPT) for more accurate translations
2.  LLMs \> Word Embeddings
    a.  Vectors: polysemy (e.g., 'bank')
    b.  Vectors: context
    c.  Vectors: precision (300 vs. 175B parameters)

<!-- # Tactic #3: Putting the Constructed in Reconstructed Language: the Slice Technique -->

<!-- ## Slice Technique -->

<!-- -   This semester, I'm working with a group of CS undergrads and a CS professor on a different strategy to arrive at the same goal -->

<!-- -   Working to produce hyperspaces with modern methods (LLMs) -->

<!-- -   LLMs use enormous corpora to produce extraordinarily detailed hyperspaces -->

<!-- -   This is a problem for ancient languages, of course! -->

<!-- ## Slice Technique: the Main Idea -->

<!-- ![](pmkn_pie.jpg){fig-align="center"} -->

<!-- ## Slice Technique: the Main Idea -->

<!-- -   Using LLMs, generate a complete hyperspace of English on full corpus -->

<!-- -   Identify how small of a corpus you need to reproduce similar hyperspace -->

<!-- -   Repeat process for other languages -->

<!-- -   Identify, broadly speaking, the size and type of corpus you need for accurate hyperspace -->

<!-- ## Slice Technique: the Controversial Part -->

<!-- ![](there_be_dragons.png){fig-align="center"} -->

# Recap & Future Directions

# Recap: WordNet

1.  Upsides: semi-universal structure
2.  Downsides: must be done manually; requires scholars to make choices that are sometimes unknowable; isn't capable of showing certain types of semantic similarities/differences beyond synonymy, hyponymy, etc.

![](PIE_wordnet.png){fig-align="center"}

# Recap: Word Embeddings

1.  Upsides: fully automated, requires low CPU processing
2.  Downsides: doesn't distinguish multiple senses (polysemy), is less accurate than LLMs
3.  Unanswered question -- can Anton's reconstruction methodologies be extended to LLM hyperspaces?

<!-- # Recap: Slice Technique -->

<!-- 1.  Upsides: utilizes LLMs, identifies how limited a corpus we need to generate hyperspace -->

<!-- 2.  Downsides: requires that we create a PIE corpus -- but how big?!? -->

<!-- ![](pmkn_pie.jpg){fig-align="center"} -->

# Future Directions: another possibility

-   [Johannson and Pietro Nina 2015](https://aclanthology.org/N15-1164.pdf "Embedding a Semantic Network in a Word Space"): build hyperspaces from systems like WordNet

    -   We should be able to do this for many IE languages (mostly modern)

    -   For languages without WordNets (like PIE), we "translate" the lexicon (\< DERBi PIE) into a WordNet structure

    -   Eliminate any matchings that are untrue (like drive "hit a golfball")

    -   Manually assign outliers as hyponyms, hypernyms, synonyms, etc. of existing lexemes
