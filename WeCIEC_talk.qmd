---
title: "Representing Semantic Relationships in Ancient IE Languages" 
subtitle: "A Pilot Study"
author: "Anton Vinogradov, Gabriel Wallace, and Andrew Byrd"
institute: "University of Kentucky" 
date: "Friday, October 25, 2024"
format:
  revealjs:
    self-contained: true
    incremental: true
    extensions:
      - qrcode
    header-includes:
      - '<img src="Horsey.jpg" style="position: absolute; top: 10px; right: 10px; width: 100px; height: auto;">'
---

## Overview of Talk

::: {style="display: flex; justify-content: center;"}
{{< qrcode https://github.com/Winjapati/weciec_2024/tree/main/WeCIEC_talk_files qr1 width=300 height=300 colorDark='#0011bb' >}}
:::

-   What led us to do this?
-   Overview of Word Embeddings
-   Reconstructing Word Embeddings using Descendant Languages
-   Future Directions & DERBi PIE

## Anton sends his regrets

[![](Anton.png){fig-align="center"}](https://linktr.ee/garyyo)

::: {style="display: flex; justify-content: center;"}
<i>*Dr. Anton Vinogradov,(recent!) PhD, Computer Science*</i>
:::

# What led us to do this?

## DERBi PIE

![](Horsey.jpg){fig-align="center"}

::: {style="display: flex; justify-content: center;"}
<i>*Database of Etymological Roots Beginning in PIE*</i>
:::

## DERBi PIE

::: fragment
-   Etymological database, with multiple, linked references
    -   all of LIV parsed (thanks Thomas Olander!)
    -   half of Pokorny fully parsed (will finish next July, provided funding)
    -   will finish parsing NIL this December
    -   ultimately: everything (?!?)
-   Have applied for NEH funding, hopefully this will put us closer to the goal of releasing to public a year from now.
:::

## DERBi PIE: Query Searches

::: fragment
-   Search Functions Created (or we know how to)
    -   Integrated Texts - identify roots, stems, and words in texts
    -   Phonological Search - identify roots, stems, and words by phonological shape (regex)
    -   Morphological Search - identify roots, stems, and words by morphological property (POS, class, gender, etc.)
:::

## DERBi PIE: Query Searches

<br>

::: fragment
-   Quickly realized that to identify semantic categories and relationships -- especially ones that make sense across languages -- is ***not*** an easy task
    -   Given cross-linguistic variation, there is no unified classification of "words"
:::

## DERBi PIE: What could this do for us?

-   In DERBi PIE, having an integrated semantic system could provide automated answers to questions such as:

    -   Are certain sound sequences associated with certain meanings or semantic spheres?

        -   Sound symbolism, etc.

    -   Morphological classes or derivations?

    -   How have meanings changed over time into the various branches and daughter languages?

## DERBi PIE: the Problem

<br> <br> <br>

<p style="text-align: center;">

-   How to create a system of semantic properties and relationships that translates across IE languages?

# Overview of Word Embeddings

## Word Embeddings: Overview

-   Second approach is one more in line with what is done in present-day NLP – identifying semantic relationships through word embeddings.

-   To do so, we must:

    -   Process a corpus for tokens and then lemmas;

    -   Analyze the environments in which these lemmas occur;

    -   Take this information to construct a semantic "hyperspace"

## Word Embeddings: Hyperspace

![Plots like these can be created with generated vectors!](beowulf.png){fig-align="center"}

## Word Embeddings: Hyperspace...?

-   If you were to plot out every vector generated from one of these models, you would have a *hyperspace* or a *semantic space*, with the dimensions of each word vector essentially acting as coordinates.

-   The closer two words lie to each other in this space, the closer in semantic value they are.

-   It's possible to adjust how many dimensions you generate for each vector, but in general the more the better.

    -   More fancy math allows us to 'simplify' these vectors into 2 or 3 for easy viewing, as seen before.

## Word Embeddings: Processing the Text for Tokens

![](Gallia%20est%20omnis%20divisa%20in%20partes%20tres%20(1).png){fig-align="center"}

## Word Embeddings: Processing the Text for Lemmas

![](lemmas.png){fig-align="center"}

## Word Embeddings: Identifying the Context

![](context.png){fig-align="center"}

## Word Embeddings: Constructing the Hyperspace

-   Using a word embeddings model such as word2vec or fastText, run the tokenized and lemmatized text through it to generate word vectors.
-   These vectors will be based on the words' positions within the text.

## Word Embeddings: Constructing the Hyperspace

-   Another Latin example:

![](vir_vectors.png){fig-align="center" width="438"}

## Word Embeddings: Calculate Word Similarities

-   To get a numerical representation of the similarity between two vectors, we use their *cosine similarity* (cosine of the angle between the vectors).
-   Similarity scores range from -1 to 1, with -1 indicating opposite vectors and 1 indicating proportional vectors.

![Fancy formula from Wikipedia](formula.png){fig-align="center"}

## Word Embeddings: Example of Cosine Similarity

![](cosine.png){fig-align="center"}

## Word Embeddings: Reconstructing a Hyperspace in Languages without a Corpus

-   We can construct a hyperspace for ancient languages;
-   But how does one do this for languages without any known corpus, such as PIE?
-   Well, how do we identify other properties of PIE?

## Word Embeddings : Basic Idea

<br>

::: {style="display: flex; justify-content: center;"}
![](comp_meth_sound.png){fig-align="center"}
:::

-   We take two, similar properties in two (or more) related languages, which allow us to approximate an earlier state in a source language

## Word Embeddings : Basic Idea

::: {style="display: flex; justify-content: center;"}
![](comp_meth_hyperspace.png){fig-align="center"}
:::

-   It is in this way that we propose that the use of word embedding models created by descendant languages, to approximate an earlier state in the source language

# Reconstructing Word Embeddings using Descendant Languages

## Word Embeddings: Methods

::: {style="display: flex; justify-content: center;"}
{{< qrcode https://github.com/Winjapati/weciec_2024/tree/main/WeCIEC_talk_files qr1 width=300 height=300 colorDark='#0011bb' >}}
:::

-   As you can imagine, this stuff is complicated, which is why we won't go into much detail about the specific methods;
    -   see Github for four-page paper, code, data, etc.
-   If there are any questions that we can't answer, we'll forward them to Anton, who will be happy to do so

## Word Embeddings, Problem #1: Verification

<br>

-   So we take descendant hyperspaces, and reconstruct an earlier, source hyperspace based on the hyperspaces provided
-   But how can we trust this methodology?
    -   Obviously can't verify the hyperspace of PIE through analysis of PIE texts!

## Word Embeddings, Problem #1: Verification

![](comp_meth_romance.png){fig-align="center"}

## Word Embeddings, Problem #2: Vectors Across Models

-   Vectors generated for hyperspaces take on arbitrary values when training models
    -   So 'dog' could = (0, 0) or (-6, 100) – these values change every time you run the model
-   For this reason, we must *align* models ([Dev et al., 2021](https://arxiv.org/pdf/1806.01330 "Closed Form Word Embedding Alignment")):
    -   Identify substructures across language models that remain fixed
    -   Use pre-aligned word embedding models (following [Joulin et al., 2018](https://arxiv.org/pdf/1804.07745 "Loss in Translation: Learning Bilingual Word Mapping with a Retrieval Criterion"))

## Word Embeddings: Methods

-   Using existing aligned models of Spanish & French ([Joulin et al., 2018](https://arxiv.org/pdf/1804.07745 "Loss in Translation: Learning Bilingual Word Mapping with a Retrieval Criterion")) as a source of vector and word information:
    -   We take aligned word vectors from Facebook AI Research's fastText ([Bojanowski et al., 2017](https://arxiv.org/pdf/1607.04606 "Enriching Word Vectors with Subword Information"))
-   Words filtered out:
    -   If there is no corresponding word in the other languages
    -   Non-vocabulary, including words with non-language characters

## Word Embeddings: Methods

<br>

![](la_wikipedia.png){fig-align="center"}

-   Models trained on French & Spanish Wikipedia articles

    -   Words removed (7.3% in French, 6.6% in Spanish)

    -   Remaining words lemmatized, furthering reducing vocabulary by roughly 10%

## Word Embeddings: Methods

<br>

-   To relate words together and find common words:
    -   Words are translated into each other's respective languages, using Google Translate, with a Python translation library deep-translator
    -   The same is done with Latin, using the Latin corpus (from [CLTK](https://cltk.org/ "The Classical Language Toolkit (Python library)") \[the Tesserae Project\]), which is lemmatized
-   Any word that cannot be lemmatized in Latin (such as Greek words) is removed from the corpus

## Word Embeddings: Calculating \*Latin

::: {style="font-size: 0.75em;"}
1a. If there's a 1:1 correspondence between the Romance language & Latin: **language-word center**

-   French *être* : Latin *esse*
:::

![](centroid.png){fig-align="center" width="175"}

::: {style="font-size: 0.75em;"}
1b. If there isn't a 1:1 correspondence, identify the centroid of lemma's vector: **language-word center**

-   French *frêle*, *fragile* : Latin *fragilis*
:::

2.  Identify the centroid of both **lwc**s -\> **inter-language-word center**

## Word Embeddings: Calculating \*Latin

::: {style="font-size: 0.9em;"}
-   With a better model/better translations, this is all we'd need.
-   Two extra steps were added by Anton to take into account translation errors and unexpected outliers.

3.  We identify the closest vectors to the **ilwc** using cosine-distance.

![](formula.png){fig-align="center"}

4.  Take the average of these two vectors to arrive at the approximate \*Latin word vector
:::

## Word Embeddings: the "Normal" Model

-   To identify the effectiveness of the method, recall that we want to compare the \*Latin with the Latin
-   This is our "normal" model

![](comp_meth_romance.png){fig-align="center"}

## Word Embeddings: How to Assess the Goodness of Your Model

-   The analogy task ([Mikolov et al. 2013](https://arxiv.org/pdf/1301.3781 "Efficient Estimation of Word Representations in Vector Space")) is considered standard when evaluating word embedding models: *London is to England as Paris is France*
-   But it doesn't work for languages with small corpora (LRLs), *especially* ones that aren't modern

## Word Embeddings: OddOneOut (Stringham & Izbicki 2020)

-   We follow [Stringham & Izbicki 2020](https://aclanthology.org/2020.eval4nlp-1.17.pdf "Evaluating Word Embeddings on Low-Resource Languages") in using the OddOneOut task, which has been demonstrated to be more accurate in these situations
-   OddOneOut task demonstrated to work for corpora as small as 1800 tokens (Old Gujarati)

## Word Embeddings: OddOneOut (Stringham & Izbicki 2020)

-   The OddOneOut technique "measures a model's ability to identify words that are unrelated to \[a specific\] category"

![](bert_pigeon.jpg){fig-align="center"}

::: {style="display: flex; justify-content: center;"}
<i>One of these things is not like the other!</i>
:::

## Word Embeddings: OddOneOut (Stringham & Izbicki 2020)

-   Categories are automatically drawn from Wikidata and implemented when applicable.
-   Example categories include (ibid., 181):
    -   **Human Biblical Figures**: *Abednego*, *Abraham*, *Azor*, *Chedorlaomer*, *Christ*, *Goliath*, *Hilkiah*, *Lo-Ammi*, *Matthew the Apostle*, *Peter*, *Sheba*, *Uz*, *Yael*, *Zerubbabel*
    -   **Hinduism**: *Adharma*, *Ashvadamedha*, *Brahmin*, *Bhagavan*, *Guru*, *Hindu Prayer Beads*, *Karma*, *Mahātmā*, *Mudra*, *Nirvana*

## Word Embeddings: Results

-   Our results indicate mixed success: we can create a hyperspace from the descendant languages that performs reasonably well on Latin tests with OddOneOut
-   Preliminary tests show that the smaller the corpus, the better the model works, as compared to the normal one

[![](heatmap.png){fig-align="center" width="392"}](https://github.com/Winjapati/weciec_2024/tree/main/WeCIEC_talk_files)

## Word Embeddings: Results

-   This is not exactly a bad thing, as this is the case for many languages in our discipline!
    -   If a language has a large corpus (such as Ancient Greek), we can reduce it as we've done here for Latin
    -   To train such models, reductions of tokens are done randomly, both with regards to the number of tokens and specific tokens used

## Word Embeddings: Results

<br>

-   Unclear what is the "magic number" for the size of corpus to arrive at most accurate representation (compared to the normal model)
-   Unclear exactly why performance of descendant models tends to decrease as the corpus size increases
    -   Anton has ideas, all model-/programming-specific -- see the draft for further

# Future Directions & DERBi PIE

## Recap: Descendant Model

<br>

-   In this talk, we implement the Comparative Method to reconstruct the hyperspaces of languages without corpora
-   Upsides:
    1.  fully automated
    2.  requires low CPU processing
-   Downsides:
    1.  doesn't distinguish multiple senses (polysemy)
    2.  only performs well on smaller corpora (compared to normal model)

## Word Embeddings: Problems with Current Model (Anonymous NLP reviewers)

1.  Use of Google translate suboptimal, may result in translation errors; should use either bilingual dictionaries or LLMs (think GPT) for more accurate translations
2.  LLMs \> Word Embeddings
    a.  Vectors: polysemy (e.g., 'bank')
    b.  Vectors: context
    c.  Vectors: precision (300 vs. 175B parameters)

## Future Directions: using LLM models

-   We believe that our reconstruction model shows promise

-   We plan to stick with the Descendant Model strategy, but:

    -   Utilize LLMs (such as GPT) for modelling (hyperspace) for greater precision and differentiation of polysemy

    -   Instead of Google Translate, use bilingual dictionary or LLM

        -   We've had great success with LLMs in the parsing of data for DERBi PIE

## Future Directions: using LLM models

-   Add additional Romance languages;

-   When happy with results, move on to other subbranches (likely Slavic or Indic) for testing;

-   We believe that a good, workable model should be able to generate a \*Proto-vector through the centroid of the language-word centers (minus those additional steps).

## Future Directions: Integration into DERBi PIE

-   Let's finish with where we began -- DERBi PIE

-   With aligned hyperspaces generated for each descendant and reconstructed language, our models would give us coordinates for each lexeme in a language

![](beowulf.png){fig-align="center"}


## Future Directions: Integration into DERBi PIE

-   Possible question: "How semantically similar are PIE *\*sC-* roots -- as compared to related *\*C-* roots?" (\**(s)peḱ-* 'look at')

    -   Calculate cosine similarities!

<!-- -->

-   Example from English:

    -   *bl-* words: "blue", "blaze", "bland", "blush", "blink", "blow", "blast", "blot", "blend", "bleak"

    -   These words have a cosine similarity score of .8016 (remember: perfect score is 1.0!)

## Thank you!

![](uky_ling.jpeg){fig-align="center" width="691"}

![](uky_CS.png){fig-align="center" width="470"}

## Download: Slides, Paper, Code

<br>

::: {style="display: flex; justify-content: center;"}
{{< qrcode https://github.com/Winjapati/weciec_2024/tree/main/WeCIEC_talk_files qr1 width=400 height=400 colorDark='#0011bb' >}}
:::
