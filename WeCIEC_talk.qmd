---
title: "Representing Semantic Relationships in Ancient IE Languages" 
subtitle: "A Pilot Study"
author: "Anton Vinogradov, Gabriel Wallace, and Andrew Byrd"
institute: "University of Kentucky" 
date: "Friday, October 25, 2024"
format: 
  revealjs:
    incremental: false
    extensions:
      - qrcode
editor: visual
---

## Download: Slides, Paper, Data

<br>

::: {style="display: flex; justify-content: center;"}
{{< qrcode https://github.com/Winjapati/weciec_2024/tree/main/WeCIEC_talk_files qr1 width=400 height=400 colorDark='#0011bb' >}}
:::

## Overview of Talk

-   What led us to do this?
-   Tactic #1: WordNet
-   Tactic #2: Reconstructing Word Embeddings using Descendant Languages
-   Tactic #3: Putting the Constructed in Reconstructed Languages
-   Future Directions

## Anton sends his regrets

[![](Anton.png){fig-align="center"}](https://linktr.ee/garyyo)

::: {style="display: flex; justify-content: center;"}
<i>*Dr. Anton Vinogradov,(recent!) PhD, Computer Science*</i>
:::

# What led us to do this?

## DERBi PIE

![](Horsey.jpg){fig-align="center"}

::: {style="display: flex; justify-content: center;"}
<i>*Database of Etymological Roots Beginning in PIE*</i>
:::

## DERBi PIE

::: fragment
-   Etymological database, with multiple references.
    -   all of LIV parsed (thanks Thomas Olander!)
    -   half of Pokorny parsed (will finish next July, provided funding)
    -   will finish parsing NIL this December
    -   ultimately: everything (?!?)
-   Have applied for NEH funding, hopefully this will put us closer to the goal of releasing to public a year from now.
:::

## DERBi PIE: Query Searches <!--# Insert DERBi PIE wheel header -->

::: fragment
-   Search Functions Created
    -   Integrated Texts - identify roots, stems, and words in texts
    -   Phonological Search - identify roots, stems, and words by phonological shape (regex)
    -   Morphological Search - identify roots, stems, and words by morphological property (POS, class, gender, etc.)
-   Quickly realized that to identify semantic categories and relationships -- especially ones that make sense across languages -- is ***not*** an easy task
    -   there is no unified classification of semantics
:::

## DERBi PIE: What could this do for us? <!--# Insert DERBi PIE wheel header -->

-   In DERBi PIE, having an integrated semantic system such as this would allow us to provide automated answers to questions such as:

    -   Are certain sound sequences associated with certain meanings or semantic spheres?

    -   Are certain morphological derivations associated with certain meanings or semantics spheres?

    -   How have meanings changed over time into the various branches and daughter languages?

## DERBi PIE: the Problem <!--# Insert DERBi PIE wheel header -->

<br> <br> <br>

<p style="text-align: center;">

This is the problem: how on earth can we do this?

</p>

<br>

<p style="text-align: center;">

And *can* we do this?

</p>

# Tactic #1: WordNet

## WordNet: What is it?

::: {style="display: flex; justify-content: center;"}
[![](wordnet_princeton.png)](https://wordnet.princeton.edu/)
:::

-   A large, organized lexical database of English words
-   Groups words into "synsets" (sets of synonyms) based on meanings

## WordNet: What is it?

::: {style="display: flex; justify-content: center;"}
[![](wordnet_princeton)](https://wordnet.princeton.edu/)
:::

-   Provides relationships between words, such as:
    -   **Synonyms** (similar meanings, e.g. "dog", "pooch")
    -   **Antonyms** (opposite meanings, e.g. "bad", "good")
    -   **Hypernyms** (general terms, e.g., "animal" for "dog")
    -   **Hyponyms** (specific terms, e.g., "dog" for "animal")

## WordNet: Linked WordNets for Ancient Indo-European languages (Zanchi & Ginevra)

::: {style="display: flex; justify-content: center;"}
[![](WordNets){width="90%"}](https://sites.google.com/unipv.it/linked-wordnets/wordnets?authuser=0)
:::

## WordNet: Building One for PIE?

::: {style="display: flex; justify-content: center;"}
![](PIE_wordnet.png)
:::

-   Working with a team of UK CS undergraduates, we mapped a list of PIE roots and words (primarily from Pokorny) onto the English WordNet structure
-   1500 roots successfully mapped, though many are false matches ('golf'?)

## WordNet: Building One for PIE?

-   Numerous substantive difficulties with entries that do not map neatly:
    -   *\*aghlu- (IEW*, p. 8) 'dark cloud; rainy weather': new hyponym (of both 'cloud' and 'weather') needed
    -   *\*ab- (IEW*, p. 1) 'water, river': new hypernym needed? or two mappings?
-   The general idea of English (or any other language) \|\> PIE is flawed, because PIE ≠ English!

## WordNet: Broader Problems

-   **Limited Scope of Meanings**: Doesn’t capture all nuances of word usage
-   **Lack of Context**: Doesn’t account for how context alters word meaning
-   **Not All Languages Have WordNet**: WordNets in some languages are better developed than in others
-   **MUST BE DONE MANUALLY**

# Tactic #2: Reconstructing Word Embeddings using Descendant Languages

## Word Embeddings

-   **Distributional Analysis:** Linguistic items with similar distributions have similar meanings
-   **Vectors:** These are generated based on a given word's distribution in a corpus
-   **Use:** The more similar any two given word's vectors are to each other, the closer those words are in meaning

## Word Embeddings

-   **Example:**
    -   Take a text, like *Beowulf*
    -   Pre-process it (tokenization, lemmatization, etc.)
    -   Generate vectors (word2vec, fastText, etc.)
    -   Compare word vectors through cosine similarities (semantic relatedness)
    -   Results!

## Word Embeddings

![Plots like these can be created with generated vectors!](beowulf.png){fig-align="center"}

## Word Embeddings: Methods

-   X

## Word Embeddings: Methods

-   X

## Word Embeddings: Results

-   X

## Word Embeddings: Results

-   X

## Word Embeddings: Problems (anonymous reviews from experts in NLP)

1.  Use of Google translate suboptimal, may result in translation errors; should use either bilingual dictionaries or LLMs (think GPT) for more accurate translations
2.  LLMs \> Word Embeddings
    1.  Vectors: polysemy (e.g., 'bank')
    2.  Vectors: context
    3.  Vectors: precision (100 vs. 175B parameters)

# Tactic #3: Putting the Constructed in Reconstructed Language: the Slice Technique

## Slice Technique

-   This semester, I'm working with a group of CS undergrads and a CS professor on a different strategy to arrive at the same goal

-   Working to produce hyperspaces with modern methods (LLMs)

-   LLMs use enormous corpora to produce extraordinarily detailed hyperspaces

-   This is a problem for ancient languages, of course!

## Slice Technique: the Main Idea

![](pmkn_pie.jpg){fig-align="center"}

## Slice Technique: the Main Idea

-   Using LLMs, generate a complete hyperspace of English on full corpus

-   Identify how small of a corpus you need to reproduce similar hyperspace

-   Repeat process for other languages

-   Identify, broadly speaking, the size and type of corpus you need for accurate hyperspace

## Slice Technique: the Controversial Part

![](there_be_dragons.png){fig-align="center"}

## Slice Technique: Status

We've just started working on this, so no results yet.

# Recap & Future Directions

# Recap: WordNet

1.  Upsides: semi-universal structure
2.  Downsides: must be done manually; requires scholars to make choices that are sometimes unknowable; isn't capable of showing certain types of semantic similarities/differences beyond synonymy, hyponymy, etc.

![](PIE_wordnet.png){fig-align="center"}

# Recap: Word Embeddings

1.  Upsides: fully automated, requires low CPU processing
2.  Downsides: doesn't distinguish multiple senses (polysemy), is less accurate than LLMs
3.  Unanswered question -- can Anton's reconstruction methodologies be extended to LLM hyperspaces?

# Recap: Slice Technique

1.  Upsides: utilizes LLMs, identifies how limited a corpus we need to generate hyperspace
2.  Downsides: requires that we create a PIE corpus -- but how big?!?

![](pmkn_pie.jpg){fig-align="center"}

# Future Directions: another possibility

-   [Johannson and Pietro Nina 2015](https://aclanthology.org/N15-1164.pdf "Embedding a Semantic Network in a Word Space"): build hyperspaces from systems like WordNet

    -   We should be able to do this for many IE languages (mostly modern)

    -   For languages without WordNets (like PIE), we "translate" the lexicon (\< DERBi PIE) into a WordNet structure

    -   Eliminate any matchings that are untrue (like drive "hit a golfball")

    -   Manually assign outliers as hyponyms, hypernyms, synonyms, etc. of existing lexemes
