---
title: "Representing Semantic Relationships in Ancient IE Languages" 
subtitle: "A Pilot Study"
author: "Anton Vinogradov, Gabriel Wallace, and Andrew Byrd"
institute: "University of Kentucky" 
date: "Friday, October 25, 2024"
format: 
  revealjs:
    incremental: false
    extensions:
      - qrcode
editor: visual
---

## To Download Slides & Draft of Word Embeddings Paper

::: {style="display: flex; justify-content: center;"}
{{< qrcode https://jmbuhr.de qr1 width=300 height=300 colorDark='#0011bb' >}}
:::

## Overview of Talk

-   What led us to do this?
-   Tactic #1: WordNet
-   Tactic #2: Reconstructing Word Embeddings using Descendant Languages
-   Tactic #3: Putting the Constructed in Reconstructed Languages
-   Future Directions

## Anton sends his regrets

[![](Anton.png){fig-align="center"}](https://linktr.ee/garyyo)

::: {style="display: flex; justify-content: center;"}
<i>*Dr. Anton Vinogradov, recent graduate of UK, PhD, Computer Science*</i>
:::

# What led us to do this?

## DERBi PIE

![](Horsey.jpg){fig-align="center"}

::: {style="display: flex; justify-content: center;"}
<i>*Database of Etymological Roots Beginning in PIE*</i>
:::

## DERBi PIE

::: fragment
-   Etymological database, with multiple references.
    -   all of LIV parsed (thanks Thomas Olander!)
    -   half of Pokorny parsed (will finish next July, provided funding)
    -   will finish parsing NIL this December
    -   ultimately: everything (?!?)
-   Have applied for NEH funding, hopefully this will put us closer to the goal of releasing to public a year from now.
:::

## DERBi PIE: Query Searches <!--# Insert DERBi PIE wheel header -->

::: fragment
-   Search Functions Created
    -   Integrated Texts - identify roots, stems, and words in texts
    -   Phonological Search - identify roots, stems, and words by phonological shape (regex)
    -   Morphological Search - identify roots, stems, and words by morphological property (POS, class, gender, etc.)
-   Quickly realized that to identify semantic categories and relationships -- especially ones that make sense across languages -- is ***not*** an easy task
:::

## DERBi PIE: Query Searches <!--# Insert DERBi PIE wheel header -->

::: {style="display: flex; justify-content: center;"}
This is the problem: how on earth to do this?
:::

# Tactic #1: WordNet

## WordNet: What is it?

::: {style="display: flex; justify-content: center;"}
[![](wordnet_princeton)](https://wordnet.princeton.edu/)
:::

-   A large, organized lexical database of English words
-   Groups words into "synsets" (sets of synonyms) based on meanings

## WordNet: What is it?

::: {style="display: flex; justify-content: center;"}
[![](wordnet_princeton)](https://wordnet.princeton.edu/)
:::

-   Provides relationships between words, such as:
    -   **Synonyms** (similar meanings, e.g. "dog", "pooch")
    -   **Antonyms** (opposite meanings, e.g. "bad", "good")
    -   **Hypernyms** (general terms, e.g., "animal" for "dog")
    -   **Hyponyms** (specific terms, e.g., "dog" for "animal")

## WordNet: Linked WordNets for Ancient Indo-European languages (Zanchi & Ginevra)

::: {style="display: flex; justify-content: center;"}
[![](WordNets){width="90%"}](https://sites.google.com/unipv.it/linked-wordnets/wordnets?authuser=0)
:::

## WordNet: Building One for PIE?

-   Working with a team of UK CS undergraduates, we mapped a list of PIE roots and words (primarily from Pokorny) onto the English WordNet structure
-   1500 roots successfully mapped

## WordNet: Building One for PIE?

-   Numerous difficulties with entries that do not map neatly:
    -   *\*aghlu- (IEW*, p. 8) 'dark cloud; rainy weather': new hyponym (of both 'cloud' and 'weather') needed
    -   *\*ab- (IEW*, p. 1) 'water, river': new hypernym needed? or two mappings?
-   The general idea of English \|\> PIE is flawed, because PIE ≠ English!

## WordNet: Broader Problems

-   **Limited Scope of Meanings**: Doesn’t capture all nuances of word usage
-   **Lack of Context**: Doesn’t account for how context alters word meaning
-   **Not All Languages Have Wordet**: WordNets in some languages are better developed than in others
-   **MUST BE DONE MANUALLY**

# Tactic #2: Reconstructing Word Embeddings using Descendant Languages

## Word Embeddings

-   **Distributional Analysis:** Linguistic items with similar distributions have similar meanings
-   **Vectors:** These are generated based on a given word's distribution in a corpus
-   **Use:** The more similar any two given word's vectors are to each other, the closer those words are in meaning

## Word Embeddings

-   **Example:**
    -   Take a text, like *Beowulf*
    -   Pre-process it (tokenization, lemmatization, etc.)
    -   Generate vectors (word2vec, fastText, etc.)
    -   Compare word vectors through cosine similarities (semantic relatedness)
    -   Results!

## Word Embeddings

![Plots like these can be created with generated vectors!](beowulf.png){fig-align="center"}

## Word Embeddings: Methods

-   X

## Word Embeddings: Methods

-   X

## Word Embeddings: Results

-   X

## Word Embeddings: Results

-   X

## Word Embeddings: Problems (anonymous reviews from experts in NLP)

1.  Use of Google translate suboptimal, may result in translation errors; should use either bilingual dictionaries or LLMs (think GPT) for more accurate translations
2.  LLMs \> Word Embeddings
    1.  Vectors: polysemy (e.g., 'bank')
    2.  Vectors: context
    3.  Vectors: precision (100 vs. 175B parameters)

# Tactic #3: Putting the Constructed in Reconstructed Language: the Slice Technique

## Slice Technique

-   This semester, I'm working with a group of CS undergrads and a CS professor on a different strategy to arrive at the same goal

-   Working to produce hyperspaces with modern methods (LLMs)

-   LLMs use enormous corpora to produce extraordinarily detailed hyperspaces

-   This is a problem for ancient languages, of course!

## Slice Technique: the Main Idea

-   The idea: generate

## Slice Technique: the Controversial Part

# Recap & Future Directions

# Recap: WordNet

1.  Upsides: semi-universal structure
2.  Downsides: must be done manually; requires scholars to make choices that are sometimes unknowable; isn't capable of showing certain types of semantic similarities/differences beyond synonymy, hyponymy, etc.

# Recap: Word Embeddings

1.  Upsides: fully automated, requires low CPU processing
2.  Downsides: doesn't distinguish multiple senses (polysemy), is less accurate than LLMs
3.  Unanswered question -- can Anton's reconstruction methodologies be extended to LLM hyperspaces?

# Recap: Slice Technique

1.  Upsides: utilizes LLMs, identifies how limited a corpus we need to generate hyperspace
2.  Downsides: requires that we create a PIE corpus -- but how big?!?

# Future Directions: another possibility

-   Johannson and Pietro Nina 2015: algorithm that converts WordNets into hyperspaces

    -   We can do this for many IE languages (mostly modern)

    -   "Translate" PIE lexicon (\< DERBi PIE) into a WordNet structure

    -   Manually assign outliers as hyponyms, hypernyms, synonyms, etc. of existing lexemes
